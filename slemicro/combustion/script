#!/bin/bash
# combustion: network
set -euo pipefail

# Redirect output to the console
exec > >(exec tee -a /dev/tty0) 2>&1

# Set hostname
echo "${VMNAME}" > /etc/hostname
hostname "${VMNAME}"

# SSH key management
if [ "${SSHCONFIG}" = true ]; then
	mkdir -pm700 /root/.ssh/
	echo "${SSHCONTENT}" >> /root/.ssh/authorized_keys
fi

# Registration
if [ "${REGISTER}" = true ]; then
	if ! which SUSEConnect > /dev/null 2>&1; then
		zypper --non-interactive install suseconnect-ng
	fi
	SUSEConnect --email "${EMAIL}" --url https://scc.suse.com --regcode "${REGCODE}"
fi

# Cockpit
if [ "${COCKPIT}" = true ]; then
	systemctl enable cockpit.socket
fi

# Podman
if [ "${PODMAN}" = true ]; then
	systemctl enable podman.service podman.socket
fi

# K3s
if [ "${CLUSTER}" == "k3s" ]; then
	# Mount /usr/local to store the k3s script
	mount /usr/local || true
	# Stolen from https://code.opensuse.org/adathor/combustion-dotconf/blob/main/f/K3s%20cluster/k3s_master/script
		# Download and install the latest k3s installer
	curl -L --output k3s_installer.sh https://get.k3s.io && install -m755 k3s_installer.sh /usr/local/bin/
	# Create a systemd unit that installs k3s if not installed yet. The k3s service is started after the installation
	cat <<- EOF > /etc/systemd/system/k3s_installer.service
	[Unit]
	Description=Run K3s installer
	Wants=network-online.target
	After=network.target network-online.target
	ConditionPathExists=/usr/local/bin/k3s_installer.sh
	ConditionPathExists=!/usr/local/bin/k3s

	[Service]
	User=root
	Type=forking
	TimeoutStartSec=600
	Environment="INSTALL_K3S_EXEC=${INSTALL_CLUSTER_EXEC}"
	Environment="INSTALL_K3S_VERSION=${INSTALL_CLUSTER_VERSION}"
	Environment="K3S_TOKEN=${CLUSTER_TOKEN}"
	Environment="INSTALL_K3S_SKIP_START=false"
	ExecStart=/usr/local/bin/k3s_installer.sh
	RemainAfterExit=yes
	KillMode=process
	# Load the proper modules for kube-vip lb to work
	ExecStartPost=/bin/sh -c "[ -f /root/ipvs.conf ] && mv /root/ipvs.conf /etc/modules-load.d/ipvs.conf || true"
	ExecStartPost=/bin/sh -c "[ -f /etc/modules-load.d/ipvs.conf ] && systemctl restart systemd-modules-load || true"
	# Move the kube-vip file if exists
	ExecStartPost=/bin/sh -c "[ -f /root/kube-vip.yaml ] && mkdir -p /var/lib/rancher/k3s/server/manifests || true"
	ExecStartPost=/bin/sh -c "[ -f /root/kube-vip.yaml ] && mv /root/kube-vip.yaml /var/lib/rancher/k3s/server/manifests/kube-vip.yaml || true"
	ExecStartPost=/bin/sh -c "[ -f /var/lib/rancher/k3s/server/manifests/kube-vip.yaml ] && chcon -t container_var_lib_t /var/lib/rancher/k3s/server/manifests/kube-vip.yaml || true"
	# Disable & delete everything
	ExecStartPost=rm -f /usr/local/bin/k3s_installer.sh
	ExecStartPost=/bin/sh -c "systemctl disable k3s_installer"
	ExecStartPost=rm -f /etc/systemd/system/k3s_installer.service
	
	[Install]
	WantedBy=multi-user.target
	EOF

	systemctl enable k3s_installer.service
fi

# RKE2
if [ "${CLUSTER}" == "rke2" ]; then
	# Mount /usr/local to store the RKE2 script
	mount /usr/local || true

	curl -L --output rke2_installer.sh https://get.rke2.io && install -m755 rke2_installer.sh /usr/local/bin/
	# Create a systemd unit that installs rke2 if not installed yet. The rke2 service is started after the installation
	cat <<- EOF > /etc/systemd/system/rke2_installer.service
	[Unit]
	Description=Run RKE2 installer
	Wants=network-online.target
	After=network.target network-online.target
	ConditionPathExists=/usr/local/bin/rke2_installer.sh
	ConditionPathExists=!/opt/rke2/bin/rke2

	[Service]
	User=root
	Type=forking
	TimeoutStartSec=600
	Environment="INSTALL_RKE2_EXEC=${INSTALL_CLUSTER_EXEC}"
	Environment="INSTALL_RKE2_VERSION=${INSTALL_CLUSTER_VERSION}"
	Environment="RKE2_TOKEN=${CLUSTER_TOKEN}"
	ExecStart=/usr/local/bin/rke2_installer.sh
	RemainAfterExit=yes
	KillMode=process
	ExecStartPost=/bin/sh -c "systemctl enable --now rke2-server.service; systemctl start --no-block --now rke2-server.service"
	# update path in exec start post to include rke2 bin path
	ExecStartPost=/bin/sh -c "echo 'export KUBECONFIG=/etc/rancher/rke2/rke2.yaml' >> ~/.bashrc ; echo 'export PATH=${PATH}:/var/lib/rancher/rke2/bin' >> ~/.bashrc ; source ~/.bashrc"
	# Load the proper modules for kube-vip lb to work
	ExecStartPost=/bin/sh -c "[ -f /root/ipvs.conf ] && mv /root/ipvs.conf /etc/modules-load.d/ipvs.conf || true"
	ExecStartPost=/bin/sh -c "[ -f /etc/modules-load.d/ipvs.conf ] && systemctl restart systemd-modules-load || true"
	# Move the kube-vip file if exists
	ExecStartPost=/bin/sh -c "mkdir -p /var/lib/rancher/rke2/server/manifests"
	ExecStartPost=/bin/sh -c "[ -f /root/kube-vip.yaml ] && mv /root/kube-vip.yaml /var/lib/rancher/rke2/server/manifests/kube-vip.yaml || true"
	ExecStartPost=/bin/sh -c "[ -f /var/lib/rancher/rke2/server/manifests/kube-vip.yaml ] && chcon -t container_var_lib_t /var/lib/rancher/rke2/server/manifests/kube-vip.yaml || true"
	# Disable & delete everything
	ExecStartPost=rm -f /usr/local/bin/rke2_installer.sh
	ExecStartPost=/bin/sh -c "systemctl disable rke2_installer"
	ExecStartPost=rm -f /etc/systemd/system/rke2_installer.service

	[Install]
	WantedBy=multi-user.target
	EOF

	systemctl enable rke2_installer.service
fi

# If Kubevip is enabled to have a VIP for the K3s API
# The official way would be:
# export VIP=192.168.205.68
# export INTERFACE=eth0
# export KVVERSION="v0.5.12"
# podman run --rm --network=host ghcr.io/kube-vip/kube-vip:$KVVERSION manifest daemonset \
#     --interface $INTERFACE \
#     --address $VIP \
#     --inCluster \
#     --taint \
#     --controlplane \
#     --arp \
#     --leaderElection > foobar.yaml
if [ "${KUBEVIP}" = true ]; then
	# lb_enable requires loading the ip_vs modules
	# https://kube-vip.io/docs/about/architecture/?query=lb_enable#control-plane-load-balancing
	# But at this point that folder is not valid and it won't work
	cat <<- EOF > /root/ipvs.conf
	ip_vs
	ip_vs_rr
	ip_vs_wrr
	ip_vs_sh
	nf_conntrack
	EOF
	
	# The proper path would be /var/lib/rancher/k3s/server/manifests
	# but as this is done at combustion time, the folder will be
	# overwritten when installing K3s as:
	# Apr 27 10:44:30 cp01 k3s_installer.sh[2437]: Warning: The following files were changed in the snapshot, but are shadowed by
	# Apr 27 10:44:30 cp01 k3s_installer.sh[2437]: other mounts and will not be visible to the system:
	# Apr 27 10:44:30 cp01 k3s_installer.sh[2437]: /.snapshots/3/snapshot/var/lib/rancher/k3s/server/manifests/kube-vip.yaml
	# So, instead, just create it somewhere and move it 
	# as an ExecStartPost in the k3s_installer service.
	# I've tried in /tmp and /var/tmp but 
	# those seem to be ephemeral at combustion time
	cat <<- EOF > /root/kube-vip.yaml
	apiVersion: v1
	kind: ServiceAccount
	metadata:
	  name: kube-vip
	  namespace: kube-system
	---
	apiVersion: rbac.authorization.k8s.io/v1
	kind: ClusterRole
	metadata:
	  annotations:
	    rbac.authorization.kubernetes.io/autoupdate: "true"
	  name: system:kube-vip-role
	rules:
	  - apiGroups: [""]
	    resources: ["services", "services/status", "nodes", "endpoints"]
	    verbs: ["list","get","watch", "update"]
	  - apiGroups: ["coordination.k8s.io"]
	    resources: ["leases"]
	    verbs: ["list", "get", "watch", "update", "create"]
	---
	kind: ClusterRoleBinding
	apiVersion: rbac.authorization.k8s.io/v1
	metadata:
	  name: system:kube-vip-binding
	roleRef:
	  apiGroup: rbac.authorization.k8s.io
	  kind: ClusterRole
	  name: system:kube-vip-role
	subjects:
	- kind: ServiceAccount
	  name: kube-vip
	  namespace: kube-system
	---
	apiVersion: apps/v1
	kind: DaemonSet
	metadata:
	  labels:
	    app.kubernetes.io/name: kube-vip-ds
	    app.kubernetes.io/version: v0.5.12
	  name: kube-vip-ds
	  namespace: kube-system
	spec:
	  selector:
	    matchLabels:
	      app.kubernetes.io/name: kube-vip-ds
	  template:
	    metadata:
	      labels:
	        app.kubernetes.io/name: kube-vip-ds
	        app.kubernetes.io/version: v0.5.12
	    spec:
	      affinity:
	        nodeAffinity:
	          requiredDuringSchedulingIgnoredDuringExecution:
	            nodeSelectorTerms:
	            - matchExpressions:
	              - key: node-role.kubernetes.io/master
	                operator: Exists
	            - matchExpressions:
	              - key: node-role.kubernetes.io/control-plane
	                operator: Exists
	      containers:
	      - args:
	        - manager
	        env:
	        - name: vip_arp
	          value: "true"
	        - name: port
	          value: "6443"
	        - name: vip_interface
	          value: eth0
	        - name: vip_cidr
	          value: "32"
	        - name: cp_enable
	          value: "true"
	        - name: cp_namespace
	          value: kube-system
	        - name: vip_ddns
	          value: "false"
	        - name: vip_leaderelection
	          value: "true"
	        - name: vip_leaseduration
	          value: "30"
	        - name: vip_renewdeadline
	          value: "20"
	        - name: vip_retryperiod
	          value: "4"
	        - name: address
	          value: ${VIP}
	        - name: lb_enable
	          value: "true"
	        - name: prometheus_server
	          value: :2112
	        image: ghcr.io/kube-vip/kube-vip:v0.5.12
	        imagePullPolicy: Always
	        name: kube-vip
	        securityContext:
	          capabilities:
	            add:
	            - NET_ADMIN
	            - NET_RAW
	      hostNetwork: true
	      nodeSelector:
	        node-role.kubernetes.io/master: "true"
	      serviceAccountName: kube-vip
	      tolerations:
	      - effect: NoSchedule
	        operator: Exists
	      - effect: NoExecute
	        operator: Exists
	EOF
fi

# Rancher
if [ "${RANCHERFLAVOR}" != false ]; then
	# Mount /usr/local to store the rancher install script
	mount /usr/local || true

	# Download helm as required to install rancher
	curl -fsSL https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 |bash

	# Create a script to install rancher that will be called via a systemd service
	# See https://stackoverflow.com/a/61259844 for the reason about ${q} :)
	cat <<- EOF > /usr/local/bin/rancher_installer.sh
	#!/bin/bash
	set -euo pipefail
	# Wait for cluster to be available
	until [ -f ${KUBECONFIG} ]; do sleep 2; done
	# export the kubeconfig using the right kubeconfig path depending on the cluster (k3s or rke2)
	export KUBECONFIG=${KUBECONFIG}
	# Wait for the node to be available, meaning the K8s API is available
	while ! ${KUBECTL} wait --for condition=ready node $(cat /etc/hostname | tr '[:upper:]' '[:lower:]') --timeout=60s; do sleep 2 ; done
	# https://ranchermanager.docs.rancher.com/pages-for-subheaders/install-upgrade-on-a-kubernetes-cluster
	case ${RANCHERFLAVOR} in
		"latest" | "stable" | "alpha")
			helm repo add rancher https://releases.rancher.com/server-charts/${RANCHERFLAVOR}
		;;
		"prime")
			helm repo add rancher https://charts.rancher.com/server-charts/prime
		;;
		*)
			echo "Rancher flavor not detected, using latest"
			helm repo add rancher https://releases.rancher.com/server-charts/latest
		;;
	esac

	helm repo add jetstack https://charts.jetstack.io
	# Update your local Helm chart repository cache
	helm repo update

	# Install the cert-manager Helm chart
	helm install cert-manager jetstack/cert-manager \
		--namespace cert-manager \
		--create-namespace \
		--set installCRDs=true \
		--version v1.11.1

	# https://github.com/rancher/rke2/issues/3958
	if [ "${CLUSTER}" == "rke2" ]; then
		# Wait for the rke2-ingress-nginx-controller DS to be available if using RKE2
		while ! ${KUBECTL} rollout status daemonset -n kube-system rke2-ingress-nginx-controller --timeout=60s; do sleep 2 ; done
	fi

	# Install rancher using sslip.io as hostname and with just a single replica
	helm install rancher rancher/rancher \
		--namespace cattle-system \
		--create-namespace \
		--set hostname=$(hostname -I | awk '{print $1}').sslip.io \
		--set bootstrapPassword=${RANCHERBOOTSTRAPPASSWORD} \
		--set replicas=1 \
		--set global.cattle.psp.enabled=false

	rm -f /etc/systemd/system/rancher_installer.service
	EOF

	chmod a+x /usr/local/bin/rancher_installer.sh

	# Create a systemd unit to install rancher once
	# Using "User=root" is required for some environment variables to be present
	cat <<- EOF > /etc/systemd/system/rancher_installer.service
	[Unit]
	Description=Deploy Rancher on K3S/RKE2
	Wants=network-online.target
	After=network.target network-online.target ${CLUSTER_INSTALL_SERVICE}
	ConditionPathExists=/usr/local/bin/rancher_installer.sh

	[Service]
	User=root
	Type=forking
	TimeoutStartSec=900
	ExecStart=/usr/local/bin/rancher_installer.sh
	RemainAfterExit=yes
	KillMode=process
	# Disable & delete everything
	ExecStartPost=rm -f /usr/local/bin/rancher_installer.sh
	ExecStartPost=/bin/sh -c "systemctl disable rancher_installer.service"
	ExecStartPost=rm -f /etc/systemd/system/rancher_installer.service

	[Install]
	WantedBy=multi-user.target
	EOF

	systemctl enable rancher_installer.service
fi

# Bypass rancher bootstrap
if [ "${RANCHERBOOTSTRAPSKIP}" = true ]; then
	# Mount /usr/local to store the rancher bootstrap skip script
	mount /usr/local || true

	# Install jq
	zypper --non-interactive install jq

	# Create a script that will skip all the rancher bootstrap steps
	# See https://stackoverflow.com/a/61259844 for the reason about ${q} :)
	cat <<- "EOF" > /usr/local/bin/skip-rancher-bootstrap.sh
	#!/bin/bash
	set -euo pipefail
	HOST="https://$(hostname -I | awk '{print $1}').sslip.io"
	# export the kubeconfig using the right kubeconfig path depending on the cluster (k3s or rke2)
	export KUBECONFIG=${KUBECONFIG}
	while ! ${KUBECTL} wait --for condition=ready -n cattle-system $(${KUBECTL} get pods -n cattle-system -l app=rancher -o name) --timeout=10s; do sleep 2 ; done

	# https://github.com/rancher/rke2/issues/3958
	if [ "${CLUSTER}" == "rke2" ]; then
		# Wait for the rke2-ingress-nginx-controller DS to be available if using RKE2
		while ! ${KUBECTL} rollout status daemonset -n kube-system rke2-ingress-nginx-controller --timeout=60s; do sleep 2 ; done
	fi

	# Get token
	TOKEN=$(curl -sk -X POST $${q}HOST/v3-public/localProviders/local?action=login -H 'content-type: application/json' -d '{"username":"admin","password":"${RANCHERBOOTSTRAPPASSWORD}"}' | jq -r .token)

	# Set password
	curl -sk $${q}HOST/v3/users?action=changepassword -H 'content-type: application/json' -H "Authorization: Bearer $${q}TOKEN" -d '{"currentPassword":"${RANCHERBOOTSTRAPPASSWORD}","newPassword":"${RANCHERFINALPASSWORD}"}'

	# Get API token
	APITOKEN=$(curl -sk $${q}HOST/v3/token -H 'content-type: application/json' -H "Authorization: Bearer $${q}TOKEN" -d '{"type":"token","description":"automation"}' | jq -r .token)

	curl -sk $${q}HOST/v3/settings/server-url -H 'content-type: application/json' -H "Authorization: Bearer $${q}APITOKEN" -X PUT -d "{\"name\":\"server-url\",\"value\":\"$${q}HOST\"}"
	curl -sk $${q}HOST/v3/settings/telemetry-opt -X PUT -H 'content-type: application/json' -H 'accept: application/json' -H "Authorization: Bearer $${q}APITOKEN" -d '{"value":"out"}'
	EOF

	chmod a+x /usr/local/bin/skip-rancher-bootstrap.sh

	# Create a systemd unit to run the steps after rancher has been installed
	# Using "User=root" is required for some environment variables to be present 
	cat <<- EOF > /etc/systemd/system/skip-rancher-bootstrap.service
	[Unit]
	Description=Skip Rancher Bootstrap
	Wants=network-online.target
	After=network.target network-online.target rancher_installer.service
	ConditionPathExists=/usr/local/bin/skip-rancher-bootstrap.sh

	[Service]
	User=root
	Type=forking
	TimeoutStartSec=900
	ExecStart=/usr/local/bin/skip-rancher-bootstrap.sh
	RemainAfterExit=yes
	KillMode=process
	# Disable & delete everything
	ExecStartPost=rm -f /usr/local/bin/skip-rancher-bootstrap.sh
	ExecStartPost=/bin/sh -c "systemctl disable skip-rancher-bootstrap.service"
	ExecStartPost=rm -f /etc/systemd/system/skip-rancher-bootstrap.service

	[Install]
	WantedBy=multi-user.target
	EOF

	systemctl enable skip-rancher-bootstrap.service
fi

# Elemental
if [ "${ELEMENTAL}" = true ]; then
	# Helm should be installed at this point
	# Mount /usr/local to store the rancher script
	mount /usr/local || true

	# Create a script to deploy elemental that will be called via a systemd service
	cat <<- "EOF" > /usr/local/bin/elemental_installer.sh
	#!/bin/bash
	set -euo pipefail

	# Wait for k3s to be available
	until [ -f ${KUBECONFIG} ]; do sleep 2; done
	# export the kubeconfig using the right kubeconfig path depending on the cluster (k3s or rke2)
	export KUBECONFIG=${KUBECONFIG}
	# Wait for the node to be available, meaning the K8s API is available
	while ! ${KUBECTL} wait --for condition=ready node $(cat /etc/hostname | tr '[:upper:]' '[:lower:]') --timeout=60s; do sleep 2 ; done

	# https://github.com/rancher/rke2/issues/3958
	if [ "${CLUSTER}" == "rke2" ]; then
		# Wait for the rke2-ingress-nginx-controller DS to be available if using RKE2
		while ! ${KUBECTL} rollout status daemonset -n kube-system rke2-ingress-nginx-controller --timeout=60s; do sleep 2 ; done
	fi

	# Add the official Rancher charts to install the ui-plugin operator & CRDs
	helm repo add rancher-charts https://charts.rancher.io/
	helm upgrade --create-namespace -n cattle-ui-plugin-system --install ui-plugin-operator rancher-charts/ui-plugin-operator
	helm upgrade --create-namespace -n cattle-ui-plugin-system --install ui-plugin-operator-crd rancher-charts/ui-plugin-operator-crd 

	# Wait for the operator to be up
	while ! ${KUBECTL} wait --for condition=ready -n cattle-ui-plugin-system $(${KUBECTL} get pods -n cattle-ui-plugin-system -l app.kubernetes.io/instance=ui-plugin-operator -o name) --timeout=10s; do sleep 2 ; done

	# Deploy the elemental UI plugin
	# NOTE: TABs and then spaces...
	cat <<- FOO | ${KUBECTL} apply -f -
	apiVersion: catalog.cattle.io/v1
	kind: UIPlugin
	metadata:
	  name: elemental
	  namespace: cattle-ui-plugin-system
	spec:
	  plugin:
	    endpoint: https://raw.githubusercontent.com/rancher/ui-plugin-charts/main/extensions/elemental/1.1.0
	    name: elemental
	    noCache: false
	    version: 1.1.0
	FOO

	# Or
	# helm repo add rancher-ui-plugins https://raw.githubusercontent.com/rancher/ui-plugin-charts/main
	# helm upgrade --install elemental rancher-ui-plugins/elemental --namespace cattle-ui-plugin-system --create-namespace

	while ! ${KUBECTL} wait --for condition=namesaccepted CustomResourceDefinition globalroles.management.cattle.io --timeout=10s; do sleep 2 ; done

	# Install elemental using an OCI registry
	helm upgrade --create-namespace -n cattle-elemental-system --install elemental-operator oci://registry.opensuse.org/isv/rancher/elemental/stable/charts/rancher/elemental-operator-chart

	EOF

	chmod a+x /usr/local/bin/elemental_installer.sh

	# Create a systemd unit to install elemental once
	# Using "User=root" is required for some environment variables to be present 
	cat <<- EOF > /etc/systemd/system/elemental_installer.service
	[Unit]
	Description=Deploy Elemental on Rancher on Cluster k3s/rke2
	Wants=network-online.target
	After=network.target network-online.target rancher_installer.service
	ConditionPathExists=/usr/local/bin/elemental_installer.sh

	[Service]
	User=root
	Type=forking
	TimeoutStartSec=900
	ExecStart=/usr/local/bin/elemental_installer.sh
	RemainAfterExit=yes
	KillMode=process
	# Disable & delete everything
	ExecStartPost=rm -f /usr/local/bin/elemental_installer.sh
	ExecStartPost=/bin/sh -c "systemctl disable elemental_installer.service"
	ExecStartPost=rm -f /etc/systemd/system/elemental_installer.service

	[Install]
	WantedBy=multi-user.target
	EOF

	systemctl enable elemental_installer.service
fi

# Update and reboot as required by transactional-update
if [ "${UPDATEANDREBOOT}" = true ]; then
	cat <<- EOF > /etc/systemd/system/update-and-reboot.service
	[Unit]
	Description=Reboot if required once
	Wants=network-online.target
	# If services doesn't exist is ok
	After=network.target network-online.target ${CLUSTER_INSTALL_SERVICE} rancher_installer.service skip-rancher-bootstrap.service elemental_installer.service
	[Service]
	User=root
	# Run this service the last one
	Type=oneshot
	ExecStart=transactional-update
	ExecStartPost=/bin/sh -c "systemctl disable update-and-reboot.service"
	ExecStartPost=rm -f /etc/systemd/system/update-and-reboot.service
	ExecStartPost=reboot
	RemainAfterExit=yes
	# Long timeout just in case
	TimeoutSec=3600
	[Install]
	WantedBy=multi-user.target
	EOF
	systemctl enable update-and-reboot.service
fi

# Leave a marker
echo "Configured with combustion" >> /etc/issue.d/combustion
